{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import certifi\n",
    "import requests\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import threading\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SSL_CERT_FILE'] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/Users/paramjaswal/Desktop/Dog_Breed/Data/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, label, images, labels):\n",
    "    try:\n",
    "        _image = load_img(image_path, target_size=_size)\n",
    "        image_array = img_to_array(_image)\n",
    "        images.append(image_array)\n",
    "        labels.append(label)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "def load_images(data_loc):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = os.listdir(data_loc)\n",
    "    class_names.sort()\n",
    "\n",
    "    threads = []\n",
    "\n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_loc, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for image_name in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, image_name)\n",
    "                thread = threading.Thread(target=load_image, args=(image_path, label, images, labels))\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    return np.array(images), np.array(labels), class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20580 images from 121 classes.\n"
     ]
    }
   ],
   "source": [
    "images, labels, class_names = load_images(data_location)\n",
    "print(f\"Loaded {len(images)} images from {len(class_names)} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16508 images belonging to 120 classes.\n",
      "Found 4072 images belonging to 120 classes.\n",
      "Training samples: 16508\n",
      "Validation samples: 4072\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
    "    validation_split=0.2,  # Split 20% of data for validation\n",
    "    horizontal_flip=True,  # Apply horizontal flip\n",
    "    zoom_range=0.2,  # Apply zoom\n",
    "    shear_range=0.2,  # Apply shear transformation\n",
    "    rotation_range=20,  # Rotate images up to 20 degrees\n",
    "    brightness_range=[0.8, 1.2]  # Adjust brightness\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_location,\n",
    "    target_size=(224, 224),  # Resize images to 224x224 pixels\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # Multi-class classification\n",
    "    subset='training'  # Use this generator for training\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    data_location,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use this generator for validation\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {val_generator.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(base_model, num_classes, learning_rate=0.0001):\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_fine_tune_model(base_model, train_generator, val_generator, num_classes, initial_epochs=10, fine_tune_epochs=10, fine_tune_layers=20):\n",
    "\n",
    "    model = build_and_compile_model(base_model, num_classes)\n",
    "    \n",
    "    history = model.fit(train_generator, validation_data=val_generator, epochs=initial_epochs)\n",
    "    \n",
    "    for layer in base_model.layers[-fine_tune_layers:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    history_fine_tuning = model.fit(train_generator, validation_data=val_generator, epochs=fine_tune_epochs)\n",
    "    \n",
    "    return model, history, history_fine_tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = 'inception_v3_weights.h5'\n",
    "if not os.path.exists(weights_path):\n",
    "    url = \"https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "    response = requests.get(url, verify=certifi.where())\n",
    "    with open(weights_path, 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paramjaswal/Desktop/Dog_Breed/VirtualEnv/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 471ms/step - accuracy: 0.5040 - loss: 2.3898 - val_accuracy: 0.7365 - val_loss: 0.9297\n",
      "Epoch 2/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 466ms/step - accuracy: 0.7638 - loss: 0.8101 - val_accuracy: 0.7571 - val_loss: 0.8555\n",
      "Epoch 3/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 468ms/step - accuracy: 0.7754 - loss: 0.7483 - val_accuracy: 0.7556 - val_loss: 0.8495\n",
      "Epoch 4/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 470ms/step - accuracy: 0.7923 - loss: 0.6792 - val_accuracy: 0.7630 - val_loss: 0.8671\n",
      "Epoch 5/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 458ms/step - accuracy: 0.8080 - loss: 0.6240 - val_accuracy: 0.7512 - val_loss: 0.8559\n",
      "Epoch 6/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 457ms/step - accuracy: 0.8140 - loss: 0.5930 - val_accuracy: 0.7525 - val_loss: 0.9013\n",
      "Epoch 7/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 467ms/step - accuracy: 0.8211 - loss: 0.5753 - val_accuracy: 0.7522 - val_loss: 0.8920\n",
      "Epoch 8/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 473ms/step - accuracy: 0.8322 - loss: 0.5274 - val_accuracy: 0.7517 - val_loss: 0.8650\n",
      "Epoch 9/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 472ms/step - accuracy: 0.8386 - loss: 0.4949 - val_accuracy: 0.7490 - val_loss: 0.8715\n",
      "Epoch 10/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 477ms/step - accuracy: 0.8516 - loss: 0.4740 - val_accuracy: 0.7596 - val_loss: 0.9041\n",
      "Epoch 1/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 502ms/step - accuracy: 0.8561 - loss: 0.5817 - val_accuracy: 0.7797 - val_loss: 0.7769\n",
      "Epoch 2/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 482ms/step - accuracy: 0.8654 - loss: 0.4837 - val_accuracy: 0.7805 - val_loss: 0.7853\n",
      "Epoch 3/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 488ms/step - accuracy: 0.8674 - loss: 0.4526 - val_accuracy: 0.7719 - val_loss: 0.7968\n",
      "Epoch 4/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 773ms/step - accuracy: 0.8684 - loss: 0.4432 - val_accuracy: 0.7773 - val_loss: 0.7941\n",
      "Epoch 5/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 501ms/step - accuracy: 0.8639 - loss: 0.4391 - val_accuracy: 0.7797 - val_loss: 0.7739\n",
      "Epoch 6/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 489ms/step - accuracy: 0.8674 - loss: 0.4391 - val_accuracy: 0.7758 - val_loss: 0.7906\n",
      "Epoch 7/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 484ms/step - accuracy: 0.8791 - loss: 0.4020 - val_accuracy: 0.7797 - val_loss: 0.7676\n",
      "Epoch 8/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 487ms/step - accuracy: 0.8723 - loss: 0.4137 - val_accuracy: 0.7760 - val_loss: 0.7902\n",
      "Epoch 9/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 487ms/step - accuracy: 0.8786 - loss: 0.4070 - val_accuracy: 0.7746 - val_loss: 0.7832\n",
      "Epoch 10/10\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 487ms/step - accuracy: 0.8808 - loss: 0.3988 - val_accuracy: 0.7733 - val_loss: 0.7861\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Load the base model\n",
    "base_model_inceptionv3 = InceptionV3(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "base_model_inceptionv3.load_weights(weights_path)\n",
    "\n",
    "model_inceptionv3, history_inceptionv3, history_fine_tuning_inceptionv3 = train_and_fine_tune_model(base_model_inceptionv3, train_generator, val_generator, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VirtualEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
